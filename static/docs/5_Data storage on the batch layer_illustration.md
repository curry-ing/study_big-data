# 5. 일괄처리 계층의 데이터 저장소: 사례

> 학습목표
- 하둡 분산 파일시스템 사용하기
- 데이터 집합 조작을 상위 수준으로 추상화하는 페일

## 5.1 HDFS 사용하기

#### HDFS 간단 복습
- [x] 파일은 블록으로 쪼개져서 클러스터의 여러 노드로 분산
- [x] 블록은 여러 노드로 복제: 내결함성
- [x] 네임노드는 각 파일이 어떤 블록으로 구성되는지와 그 블록이 어느 데이터노드에 저장되는지 추적

### 5.1.1 작은 파일 문제
- 하둡은 데이터가 작은 크기로 여러 파일이 저장된 경우 계산 능력이 하락(10배 까지도...)
- MR작업이 입력 데이터 집합의 각 블록마다 태스크를 하나씩 실행하는데, 작은 파일이 여럿 있으면 그만큼 오버헤드가 발생
- HDFS API를 사용하는 코드를 작성하거나 전용 맵리듀스 작업을 사용해 파일 통합을 해결할 수 있지만 어렵다

### 5.1.2 상위 수준 추상화
**우아한** (원하는 작업을 간결하게 표현 가능한) 솔루션을 찾아봅시다

##### 마스터 데이터 집합 조작시 중요한 두 가지 연산
- [x] 새로운 데이터를 데이터 집합에 추가
- [x] 데이터 집합에 수직 분할을 적용하고 기존 분할이 깨지는 것 방지

##### HDFS의 요구 사항
- [ ] 작은 파일을 큰 파일로 통합할 수 있어야 한다

##### `Pail` 라이브러리
```scala
import java.io.IOException
import backtype.hadoop.pail.Pail

class PailMove {
  def mergeData(masterDir: String, updateDir: String) {
    target: Pail = new Pail(masterDir)    // 1
    source: Pail = new Pail(updateDir)
    target.absorb(source)                 // 2
    target.consolidate                    // 3
  }
}

```
1. `Pail`은 HDFS폴더를 포장한 래퍼
2. 페일 라이브러리를 사용한 추가 작업
3. 페일 내 작은 데이터 파일들을 통합하는 함수

---

- 파일 포맷이 다른 경우 올바른 파일 포맷이 되도록 강제, 수직 분할 방식이 다른 경우 예외 발생
- 파일과 디렉터리같은 하위 수준 컨테이너를 사용하지 않고 데이터를 직접 핸들링

## 5.2 페일을 사용하여 일괄 처리 계층에 데이터를 저장하기
- 페일(`pail`): 데이터 집합에 관한 메타 데이터의 명칭
- 파일과 폴더를 얇게 추상화하여 데이터 추가, 수직분할, 파일 통합등을 쉽고 안전하게 수행

### 5.2.1 페일 라이브러리 기본 연산

#### 새로운 데이터 저장
```
object simpleIO {
  def main(args: List[String]) = {
    pail: Pail = Pail.create("/Users/masunghoon/Dropbox/Study/big-data/pail/mypail")

  }
}
```
